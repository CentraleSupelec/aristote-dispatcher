services:
  rabbitmq:
    image: rabbitmq:3.13-management
    ports:
      - "5672:5672"
      - "15672:15672"

  postgres:
    image: postgres
    volumes:
      - postgres-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: test
    ports:
      - "5432:5432"

  sender:
    build:
      dockerfile: Dockerfile.sender
    command:
      - fastapi
      - dev
      - src/sender/main.py
      - --host
      - 0.0.0.0
      - --port
      - "8080"
    ports:
      - "8080:8080"
    environment:
      LOG_LEVEL: 20
      DATABASE_URL: postgresql+psycopg2://user:password@postgres:5432/test
      MODEL_HOST_MAPPING: >
        {
          "Qwen/Qwen2-0.5B-Instruct": ["centralesupelec"]
        }
    restart: on-failure
    volumes:
      - ./src/sender:/app/src/sender
      - ./alembic:/app/alembic
    depends_on:
      - rabbitmq
      - postgres

  consumer:
    build:
      dockerfile: Dockerfile.consumer
    environment:
      LOG_LEVEL: 10
      MODEL: Qwen/Qwen2-0.5B-Instruct
      VLLM_SERVERS: >
        {
          "http://host.docker.internal:8000": {
            "token": "secret_key",
            "organization": "centralesupelec",
            "max_parallel_requests": 20
          }
        }
      ROUTING_STRATEGY: round-robin
      TIME_TO_FIRST_TOKEN_THRESHOLD: 0.5
      PRIORITY_HANDLER: "ignore"
      QUALITY_OF_SERVICE_POLICY: "parallel-requests-threshold-requeue"
    restart: on-failure
    depends_on:
      - rabbitmq

  # Uncomment service below (and comment vllm-arm service) if you have an amd CPU architecture.
  # vllm-amd:
  #   image: vllm/vllm-openai:v0.9.0
  #   ports:
  #     - "8000:8000"
  #   command:
  #     - --api-key
  #     - secret_key
  #     - --scheduling-policy
  #     - priority
  #     - --trust-remote-code
  #     - --model
  #     - "Qwen/Qwen2-0.5B-Instruct"
  #   environment:
  #     - VLLM_PORT=8000

  # This service works on MacOS Apple Silicon architecture. Comment this block and uncomment vllm-amd block for other
  # architectures.
  # You need to build the arm image using the Dockerfile provided in their github repository:
  # git clone git@github.com:vllm-project/vllm.git
  # cd vllm
  # docker build -f docker/Dockerfile.arm -t vllm-cpu:latest
  vllm-arm:
    image: vllm-cpu:latest
    privileged: true
    shm_size: 6g
    ports:
      - "8000:8000"
    command:
      - --api-key
      - secret_key
      - --scheduling-policy
      - priority
      - --trust-remote-code
      - --model
      - "Qwen/Qwen2-0.5B-Instruct"
    environment:
      VLLM_PORT: 8000
      VLLM_CPU_KVCACHE_SPACE: 6
      VLLM_CPU_OMP_THREADS_BIND: 6

  # Uncomment below to test load balancing
  # second-vllm-arm:
  #   image: vllm-cpu:latest
  #   privileged: true
  #   shm_size: 6g
  #   ports:
  #     - "8001:8000"
  #   command:
  #     - --api-key
  #     - secret_key
  #     - --scheduling-policy
  #     - priority
  #     - --trust-remote-code
  #     - --model
  #     - "Qwen/Qwen2-0.5B-Instruct"
  #   environment:
  #     VLLM_PORT: 8000
  #     VLLM_CPU_KVCACHE_SPACE: 6
  #     VLLM_CPU_OMP_THREADS_BIND: 6

volumes:
  postgres-data:
