apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "vllm.fullname" . }}-consumer    
spec:
  replicas: {{ .Values.consumer.replicaCount }}
  serviceName: {{ .Release.Name }}-consumer
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "vllm.fullname" . }}-consumer
      app.kubernetes.io/instance: {{ include "vllm.fullname" . }}-consumer
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "vllm.fullname" . }}-consumer
        app.kubernetes.io/instance: {{ include "vllm.fullname" . }}-consumer
    spec:
      containers:
        - name: vllm
          command:
          - python3
          - -m
          - vllm.entrypoints.openai.api_server
          - --model
          - {{ .Values.model | quote }}
          - --quantization={{ .Values.quantization }}
          {{ if not (eq .Values.dtype "")}}
          - --dtype={{ .Values.dtype }}
          {{ end }}
          - --trust-remote-code
          image: "{{ .Values.inferenceserver.image.repository }}:{{ .Values.inferenceserver.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.inferenceserver.image.pullPolicy | quote }}
          ports:
            - name: http
              containerPort: {{ .Values.inferenceserver.port }}
              protocol: TCP
          startupProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 20
            periodSeconds: 6
            # Allow for up to 10 minutes of startup time
            failureThreshold: 100
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
          resources:
            {{- toYaml .Values.inferenceserver.resources | nindent 12 }}
          env:
            - name: PORT 
              value: {{ .Values.inferenceserver.port | quote }}
            - name: MODEL
              value: {{ .Values.model | quote }}
            {{- if .Values.servedModelName }}
            - name: SERVED_MODEL_NAME
              value: {{ .Values.servedModelName | quote }}
            {{- end }}
            {{- if .Values.quantization }}
            - name: QUANTIZATION
              value: {{ .Values.quantization | quote }}
            {{- end }}
            {{- if .Values.dtype }}
            - name: DTYPE
              value: {{ .Values.dtype | quote }}
            {{- end }}
            {{- if .Values.gpuMemoryUtilization }}
            - name: GPU_MEMORY_UTILIZATION
              value: {{ .Values.gpuMemoryUtilization | quote }}
            {{- end }}
            {{- if .Values.maxModelLen }}
            - name: MAX_MODEL_LEN
              value: {{ .Values.maxModelLen | quote }}
            {{- end }}
            - name: HF_TOKEN
              value: {{ .Values.huggingface_token | quote }}
            {{- with .Values.inferenceserver.env }}
            {{ toYaml . | nindent 12 }}
            {{- end }}
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
        - name: consumer
          image: "{{ .Values.consumer.image.repository }}:{{ .Values.consumer.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.consumer.image.pullPolicy | quote }}
          livenessProbe:
            exec:
              command:
              - sh
            initialDelaySeconds: 2
            periodSeconds: 30
            failureThreshold: 10
          readinessProbe:
            exec:
              command:
              - sh
            initialDelaySeconds: 2
            periodSeconds: 30
            failureThreshold: 10
          resources:
            {{- toYaml .Values.consumer.resources | nindent 12 }}
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: SERVICE_NAME
              value: {{ .Release.Name }}-consumer
            - name: TARGET_PORT
              value: {{ .Values.inferenceserver.port | quote }}
            - name: SUFFIX
              value: {{ .Values.inferenceserver.suffix | quote }}
            {{ if .Values.rabbitmq.enabled}}
            {{- if .Values.rabbitmq.internal }}
            - name: RABBITMQ_HOST
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-rabbitmq-default-user
                  key: host
            - name: RABBITMQ_USER
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-rabbitmq-default-user
                  key: username
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-rabbitmq-default-user
                  key: password
            {{ else }}
            - name: RABBITMQ_HOST
              value: {{ .Values.rabbitmq.host | quote}}
            - name: RABBITMQ_USER
              value: {{ .Values.rabbitmq.auth.user | quote}}
            - name: RABBITMQ_PASSWORD
              value: {{ .Values.rabbitmq.auth.password | quote}}
            {{ end }}
            {{ end }}
            {{- with .Values.inferenceserver.env }}
            {{ toYaml . | nindent 12 }}
            {{- end }}
      volumes:
         - name: dshm
           emptyDir:
              medium: Memory
