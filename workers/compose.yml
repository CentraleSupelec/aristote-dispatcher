services:
  rabbitmq:
    image: rabbitmq:3.13-management
    ports:
      - 5672:5672 # Port pour la communication AMQP
      - 15672:15672 # Port pour l'interface web de RabbitMQ

  postgres:
    image: postgres
    volumes:
      - ./init_db/init_postgres.sql.example:/docker-entrypoint-initdb.d/init.sql
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: test
    ports:
      - 5432:5432

  # mysql:
  #   image: mysql
  #   volumes:
  #     - ./init_db/init_mysql.sql:/docker-entrypoint-initdb.d/init.sql
  #   environment:
  #     MYSQL_ROOT_PASSWORD: root
  #     MYSQL_DATABASE: test
  #     MYSQL_USER: user
  #     MYSQL_PASSWORD: password
  #   ports:
  #     - "3306:3306"

  sender:
    platform: linux/amd64
    build:
      context: ./sender
    ports:
      - 8080:8080
    environment:
      LOG_LEVEL: 20
      DB_TYPE: postgresql
      DB_PORT: 5432
      DB_HOST: postgres
      DB_USER: user
      DB_PASSWORD: password
      DB_DATABASE: test
    restart: on-failure
    depends_on:
      - rabbitmq
      - postgres

  consumer:
    build:
      context: ./consumer
    environment:
      LOG_LEVEL: 10
      MODEL: Qwen/Qwen2.5-1.5B-Instruct
      VLLM_SERVERS: '{"http://vllm:8000": "optional_token_here"}'
      ROUTING_STRATEGY: less-busy
    restart: on-failure
    depends_on:
      - rabbitmq

  vllm:
    image: vllm/vllm-openai:v0.6.6
    # This image works for amd only. For arm architecture, vllm does not provide an image out-of-the-box
    # In this case you need to build the arm image using the Dockerfile provided in their github repository:
    # git clone git@github.com:vllm-project/vllm.git
    # cd vllm
    # git checkout v0.6.6
    # docker build -f Dockerfile.arm -t vllm-arm:v0.6.6
    ports:
      - 50000:8000
    entrypoint:
      - python3
      - -m
      - vllm.entrypoints.openai.api_server
      - --model
      - "Qwen/Qwen2.5-1.5B-Instruct"
      # - --quantization=quantization
      - --dtype=float16
      - --trust-remote-code
    environment:
      - VLLM_PORT=8000
      # - OMP_NUM_THREADS=4  # Limit to 4 CPU threads
      # - VLLM_CPU_MAX_THREADS=4  # (if vLLM respects this env)

  # Uncomment to test load balancing
  # vllm-b:
  #   image: vllm/vllm-openai:v0.6.6
  #   ports:
  #     - 50001:8001
  #   entrypoint:
  #     - python3
  #     - -m
  #     - vllm.entrypoints.openai.api_server
  #     - --model
  #     - "Qwen/Qwen2.5-1.5B-Instruct"
  #     # - --quantization=quantization
  #     - --dtype=float16
  #     - --trust-remote-code
  #     - --port=8001
  #   environment:
  #     - VLLM_PORT=8001
  #     # - OMP_NUM_THREADS=4  # Limit to 4 CPU threads
  #     # - VLLM_CPU_MAX_THREADS=4  # (if vLLM respects this env)

volumes:
  db-data:
